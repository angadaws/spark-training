{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching Data\n",
    "\n",
    "Spark offers the possibility to cache data, which means that it tries to keep (intermediate) results either in memory or on disk. This can be very helpful in iterative algorithms or interactive analysis, where you want to prevent that the same processing steps are performed over and over again.\n",
    "\n",
    "### Approach to Caching\n",
    "Instead of performing timings of individual executions, we use the `explain()` method again to see how output changes with cached intermediate results.\n",
    "\n",
    "### Weather Example\n",
    "We will again use the weather example to understand how caching works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "\n",
    "First we load the weather data, which consists of the measurement data and some station metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements\n",
    "\n",
    "Measurements are stored in multiple directories (one per year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", lit(2003))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Measurements\n",
    "\n",
    "Measurements were stored in a proprietary text based format, with some values at fixed positions. We need to extract these values with a simple `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    col(\"year\"),\n",
    "    substring(col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"),11,5).alias(\"wban\"),\n",
    "    substring(col(\"value\"),16,8).alias(\"date\"),\n",
    "    substring(col(\"value\"),24,4).alias(\"time\"),\n",
    "    substring(col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"),66,4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"),88,5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")\n",
    "    \n",
    "weather.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata\n",
    "\n",
    "We also need to load the weather station meta data containing information about the geo location, country etc of individual weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")\n",
    "\n",
    "# Display first 10 records    \n",
    "stations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Caching Data\n",
    "\n",
    "For analysing the impact of cachign data, we will use a slightly simplified variant of the weather analysis (only temperature will be aggregated). We will change the execution by caching intermediate results and watch how execution plans change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Original Execution Plan\n",
    "\n",
    "First let's have the execution plans of the original query as our reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_weather = weather.join(stations, (weather.usaf == stations.USAF) & (weather.wban == stations.WBAN))\n",
    "result = joined_weather.groupBy(joined_weather.CTRY, joined_weather.year).agg(\n",
    "        min(when(joined_weather.air_temperature_qual == lit(1), joined_weather.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined_weather.air_temperature_qual == lit(1), joined_weather.air_temperature)).alias('max_temp')\n",
    "    )\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Caching Weather\n",
    "\n",
    "First let us simply cache the joined input DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forcing physical caching\n",
    "\n",
    "The `cache()` method again works lazily and only marks the DataFrame to be cached. The physical cache itself will only take place once the elements are evaluated. A common and easy way to enforce this is to call a `count()` on the to-be cached DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan with Cache\n",
    "\n",
    "Now let us have a look at the execution plan with the cache for the `weather` DataFrame enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = joined_weather.groupBy(joined_weather.CTRY, joined_weather.year).agg(\n",
    "        min(when(joined_weather.air_temperature_qual == lit(1), joined_weather.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined_weather.air_temperature_qual == lit(1), joined_weather.air_temperature)).alias('max_temp')\n",
    "    )\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Although the data is already cached, the execution plan still contains all steps. But the caching step won't be executed any more (since data is already cached), it is only mentioned here for completenss of the plan. We will see in the web interface.\n",
    "\n",
    "The cache itself is presented as two steps in the execution plan:\n",
    "* Creating the cache (InMemoryRelation)\n",
    "* Using the cache (InMemoryTableScan)\n",
    "\n",
    "If you look closely at the execution plans and compare these to the original uncached plan, you will notice that certain optimizations are not performed any more:\n",
    "* Cache contains ALL columns of the weather DataFrame, although only a subset is required.\n",
    "* Filter operation of JOIN is performed part of caching.\n",
    "\n",
    "Caching is an optimization barrier. This means that Spark can only optimize plans before building the cache and plans after using the cache. No optimization is possible that spans building and using the cache. The idea simply is that the DataFrame should be cached exactly how it was specified without any column truncating or record filtering in place which appears after the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Uncaching Data\n",
    "\n",
    "Caches occupy resources (memory and/or disk). Once you do not need the cache any more, you'd probably like to free up the resources again. This is easily possible with the `unpersist()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exeuction plan after unpersist\n",
    "\n",
    "Now we'd expect to have the original execution plan again. But for some reason (bug?) we don't get that any more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = joined_weather.groupBy(joined_weather.CTRY, joined_weather.year).agg(\n",
    "        min(when(joined_weather.air_temperature_qual == lit(1), joined_weather.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined_weather.air_temperature_qual == lit(1), joined_weather.air_temperature)).alias('max_temp')\n",
    "    )\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As you see in the execution plan, the cache has been removed now and the plan equals to the original one before we started caching data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Cache Levels\n",
    "\n",
    "Spark supports different levels of cache (memory, disk and a combination). These can be specified explicitly if you use `persist()` instead of `cache()`. Cache actually is a shortcut for `persist(MEMORY_AND_DISK)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "weather.persist(StorageLevel.MEMORY_ONLY)\n",
    "weather.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "weather.persist(StorageLevel.DISK_ONLY)\n",
    "weather.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "weather.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "weather.persist(StorageLevel.MEMORY_ONLY_SER_2)\n",
    "weather.persist(StorageLevel.DISK_ONLY_2)\n",
    "weather.persist(StorageLevel.MEMORY_AND_DISK_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache level explanation\n",
    "\n",
    "* `MEMORY_ONLY` - stores all records directly in memory\n",
    "* `MEMORY_ONLY_SER` - stores all records serialized in memory. Should use less memory, but requires additional work by the CPU\n",
    "* `DISK_ONLY` - stores all records serialized on disk\n",
    "* `MEMORY_AND_DISK` - stores all records first in memory and spills onto disk when no space is left in memory\n",
    "* `..._2` - stores caches on two nodes instead of one for additional redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.3 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
